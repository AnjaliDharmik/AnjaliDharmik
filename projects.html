<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>Anjali Dharmik</title>
  <meta name="description" content="Anjali Dharmik
  Senior AI Engineer, HP Inc
  anjali.dharmik@gmail.com">
  <meta name="keywords" content="Anjali Dharmik">

  <!-- Favicons -->
  <link href="assets/img/favicon.ico" rel="icon">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">

  <!-- Main CSS File -->
  <link href="assets/css/main.css" rel="stylesheet">
  
  <style>
	  .role {
	  font-size: 1.1rem;
	  font-weight: 600;
	  color: #000;
	  display: flex;
	  justify-content: space-between;
	  align-items: center;
	  border-bottom: 1px solid #ccc;
	  padding-bottom: 0.3rem;
	  margin-bottom: 0.5rem;
	}

	.role .year {
	  font-weight: normal;
	  font-size: 0.95rem;
	  color: #555;
	}
  </style>

</head>

<body class="index-page">

  <header id="header" class="header d-flex align-items-center light-background sticky-top">
    <div class="container-fluid position-relative d-flex align-items-center justify-content-between">

      <a href="index.html" class="logo d-flex align-items-center me-auto me-xl-0">
        <h1 class="sitename">Anjali Dharmik</h1>
      </a>

      <nav id="navmenu" class="navmenu">
        <ul>
          <li><a href="index.html" class="active">Profile</a></li>
          <li><a href="publications.html">Publications</a></li>
		  <li class="dropdown"><a href="#"><span>Resume</span> <i class="bi bi-chevron-down toggle-dropdown"></i></a>
            <ul>
              <li><a href="resume.html#experience">Experience</a></li>
			  <li><a href="resume.html#education">Education</a></li>
              <li><a href="resume.html#achievements">Achievements</a></li>
              <li><a href="resume.html#certification">Certification</a></li>
			  <li><a href="resume.html#skills">Skills</a></li>
            </ul>
          </li>
          <li class="dropdown"><a href="#"><span>Projects</span> <i class="bi bi-chevron-down toggle-dropdown"></i></a>
            <ul>
			  <li><a href="projects.html#MM">MultiModel</a></li>
              <li><a href="projects.html#CV">Computer Vision</a></li>
			  <li><a href="projects.html#NLP">Natural Language Processing</a></li>
              <li><a href="projects.html#ML">Machine Learning</a></li>
              <!--<li><a href="projects.html#GAI">Generative AI</a></li>-->
              <li><a href="projects.html#LLM">LLM</a></li>
            </ul>
          </li>
          <li><a href="contact.html">Contact</a></li>
        </ul>
        <i class="mobile-nav-toggle d-xl-none bi bi-list"></i>
      </nav>

    </div>
  </header>

  <main class="main">

    <section id="CV" class="services section">

      <div class="container section-title" data-aos="fade-up">
        <h2>Computer Vision Peojects</h2>
      </div>

      <div class="container">
        <div class="row gy-4">
		  
          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/COVID-19-APP" class="stretched-link">
                <h3>COVID-19-APP</h3>
              </a>
			  <h6> Healthcare </h6>
			  <b> Tech Stack: Python, TensorFlow, Keras, CNN (DenseNet121, ResNet50, VGG16, ConvNeXtTiny, MobileNet, NASNetMobile)</b>
              <p>COVID-19 Pneumonia Diagnosis Using Medical Images (Deep Learning–Based Transfer Learning Approach)</br>Developed an automated COVID-19 diagnostic system using deep transfer learning on chest X-ray and CT images. Evaluated multiple CNN architectures (VGG16, ResNet50, ConvNeXtTiny, MobileNet, NASNetMobile, DenseNet121) for accuracy and robustness. DenseNet121 achieved the best performance with 98% accuracy, 96.9% precision, 98.9% recall, and an AUC of 99.8%, demonstrating high reliability in detecting COVID-19 cases. The study highlighted AI’s potential to enhance early disease detection and support scalable, resource-efficient medical diagnostics. </br>Published in JMIRx Med, 2025.</p>
            </div>
          </div>
		  
          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>Semantic Segmentation for Tumor and Lesion Detection in Medical Imaging</h3>
              <!--</a>-->
			  <h6> Healthcare </h6>
			  <b> Tech Stack: Python, TensorFlow, U-Net (semantic segmentation architectures)</b>
              <p>Implemented and Deployed a deep learning-based semantic segmentation system to automatically identify and delineate tumors, lesions, or other pathological regions in medical images such as MRI, CT scans, or X-rays. The model assigns a class label to each pixel, enabling precise localization and quantification of abnormal areas, which can assist radiologists in diagnosis, treatment planning, and monitoring disease progression. The project leverages publicly available datasets and advanced segmentation architectures to achieve high accuracy and reliability in real-world medical applications.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>Automated Detection of Surgical Instruments and Equipment in Operating Rooms</h3>
              <!--</a>-->
			  <h6> Healthcare </h6>
			  <b> Tech Stack: Python, YOLOv8, Faster R-CNN, OpenCV</b>
              <p>Developed a deep learning-based object detection system to identify and track surgical instruments and equipment in real-time within operating rooms. Leveraged computer vision techniques to enhance workflow efficiency, reduce human error, and improve surgical safety. Implemented models using YOLOv8, Faster R-CNN and trained on annotated medical datasets to achieve high detection accuracy.</p>
            </div>
          </div>
		  
		<div class="row gy-4">
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>Touchless Attendance System</h3>
              <!--</a>-->
			  <h6> Face Recognition </h6>
			  <b> Tech Stack: Python, TensorFlow (TF Serving, TF Lite), CNN (ResNet-50, MTCNN), FastAPI, Azure</b>
              <p>Touchless Attendance System provides robust attendance system for the employees and contract workers without any complicated hardware. We can configure this software on any android or IOS based device. Algorithms can detect faces with mask on, identify people with any change of facial attributes like beard, specs. Employees cannot mark attendance with the any ID card, only through facial screening. The data set is a collection of all employees with one Passport image and Video from all angles. Implemented Touchless Attendance System using Face Recognition Model using Convolution Neural Network using Transfer Learning RestNet-50, MTCNN model with Tensorflow (TF serving, TF Lite), create API using FastAPI and deployed model on AZURE server.</p>
            </div>
          </div>
		  
		  <div></div>

        </div>	

      </div>
	  
	  </section>
	  
	  <section id="NLP" class="services section">
	  
      <div class="container section-title" data-aos="fade-up">
        <h2>Natural Language Processing Peojects</h2>
      </div>
	  
	  <div class="container">

        <div class="row gy-4">
		
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">--> <!--https://github.com/AnjaliDharmik/Skin-Disease-Text-Classification-->
                <h3>Skin Disease Text Classification</h3>
              <!--</a>-->
			  <h6> Healthcare </h6>
			  <b> Tech Stack: Python, NLP (Spacy and NLTK), Scikit-learn, Text Classification</b>
              <p>Skin Disease Text Classification uses NLP to categorize dermatology texts for diagnosis and research. Challenges include complex terms, data scarcity, class imbalance, and privacy concerns. It aids diagnosis, clinical support, and telemedicine.</p>
            </div>
          </div>
		  
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>Luffa - Political Analysis</h3>
              <!--</a>-->
			  <h6> News Analysis </h6>
			  <b> Tech Stack: Python, NLP, Sentiment Analysis, Text Classification, Summarization, News Scraping, Visualization</b>
              <p>This is News Analysis project with Aress Software’s that aimed at improving Myanmar peace process through analysis News and Politician behaviour Analysis and simplify complex situations and find quick solutions to intractable problems by delivering different insights (Sentiment Analysis on News Headlines, News Document Classification, News Description Summarization, Fake News analysis, News Article Segmentation, News Recommendation System, Data Visualization) to analysis news with 48 different news websites and social media accounts. It was an eight-month project, and we were able to successfully develop this project on Political Consulting website and achieve excellent response from the customer.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/Text--Summarizer" class="stretched-link">
                <h3>Text Summarizer</h3>
              </a>
			  <b> Tech Stack: Python, GPT-2 Transformer, TensorFlow</b>
              <p>Developed a document summarization system using the GPT-2 Transformer architecture to generate concise and coherent summaries from long-form text. Implemented the transformer decoder (GPT-2) from scratch to explore the mechanics of abstractive summarization and enhance understanding of large language model behavior. The project leveraging the GPT-2 Transformer as the core model for efficient text representation and generation. <a href="https://www.youtube.com/watch?v=aa5k2EqKWpA" class="stretched-link">Demo</a></p>
            </div>
          </div>
		  
		  <div></div>
		  
		  </div>
		  
		  <div class="row gy-4">

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/RASA_intent_classification" class="stretched-link">
                <h3>RASA Intent Classification</h3>
              </a>
			  <b> Tech Stack: Python, RASA NLU, SpaCy, TensorFlow, BERT</b>
              <p>The RASA Intent Classification project focused on building an NLP-based model to automatically identify the intent behind user input text. Using a dataset of 15,000 labeled samples and 150 unique intents, I performed text preprocessing, including lowercasing and punctuation removal, before training models with RASA NLU and SpaCy. The system was implemented and evaluated using Python, achieving strong performance in accurately predicting user intents. I also experimented with a BERT + TensorFlow model for comparison, with the RASA–SpaCy model delivering better overall results.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">--> <!--https://github.com/AnjaliDharmik/Sentiment-Analysis-->
                <h3>Sentiment Analysis</h3>
              <!--</a>-->
			  <b> Tech Stack: Python, Scikit-learn, NLP (rule-based + ML models), Deep Learning</b>
              <p>Sentiment Analysis for Yelp reviews use NLP to classify reviews as positive or negative, helping businesses understand customer feedback. Methods include rule-based, ML models, and deep learning.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/Name-Entity-Recognition" class="stretched-link">
                <h3>Named Entity Recognition (NER) for Human Activity Captions</h3>
              </a>
			  <h6>Surveillance Research</h6>
			  <b>Tech Stack: Python, NLTK, Scikit-learn, Transformers (BERT, Phi-2, RoBERTa), Pandas, JSON</b>
              <p>Developed a Named Entity Recognition (NER) system leveraging Transformer-based models (Phi-2, BERT, RoBERTa) to extract subjects and objects from textual captions derived from the MSCOCO2017 dataset. The project focused on captions containing “a person” to capture normal human activities in text form. Built a baseline model using Phi-2 for noun extraction and fine-tuned BERT-based models for improved entity recognition. Designed a complete data pipeline including tokenization, POS tagging, IOB/BIO annotation, and custom evaluation metrics—accuracy, precision, recall, F1-score, and a novel “covered area” metric to measure partial entity extraction performance. Compared model efficiency and performance for real-world surveillance text analysis.</p>
			  
            </div>
          </div>
		  
		  <div> </div>

        </div>
		
		<div class="row gy-4">
		
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/Text-Similarity-Using-Siamese-Deep-Neural-Network" class="stretched-link">
                <h3>Text-Similarity-Using-Siamese-Deep-Neural-Network</h3>
              </a>
			  <b>Tech Stack: Python, Keras, Siamese Network Architecture</b>
              <p>Siamese neural network is a class of neural network architectures that contain two or more identical subnetworks. Identical means they have the same configuration with the same parameters and weights. Parameter updating is mirrored across both subnetworks. Instead of a model learning to classify its inputs, the neural networks learns to differentiate between two inputs. It learns the similarity between them.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">--> <!--https://github.com/AnjaliDharmik/TwitterWebScraping-->
                <h3>Social Media Web Scraping</h3>
              <!--</a>-->
			  <h6> Surveillance </h6>
			  <b>Tech Stack: Python, Selenium, BeautifulSoup, Deep Learning, NLP</b>
              <p>The Twitter and Instagram Web Scraping project involved collecting and analyzing text, video, image and audio data using Python, Selenium, BeautifulSoup, Deep Learning without requiring user login or API credentials. The goal was to extract public text, images, audio and videos for sentiment and content analysis. This system enables the collection of real-time opinions and trends related to products, services, and events. In one application, I performed sentiment analysis on text from disaster-affected areas to identify negative posts and support rapid response insights. The project demonstrates the potential of social media data mining for market research, public sentiment monitoring, and crisis management.</p>
            </div>
          </div>

        </div>

      </div>
	  
	  </section>
	  
	  <section id="MM" class="services section">

      <div class="container section-title" data-aos="fade-up">
        <h2>MultiModel Peojects</h2>
      </div>

      <div class="container">

        <div class="row gy-4">
		
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>AI-Based Image Captioning for Medical Imaging Analysis</h3>
              <!--</a>-->
			  <h6> Healthcare </h6>
			  <b>Tech Stack: Python, CNN + RNN Transformer (Vision + Text Model), TensorFlow</b>
              <p>Developed an AI-powered system that automatically generates descriptive captions for medical images such as X-rays, MRIs, and pathology slides. The project integrates convolutional neural networks (CNNs) to extract image features with recurrent neural networks (RNNs) or transformer-based models to produce accurate and meaningful textual descriptions. This approach aids healthcare professionals in rapid reporting, diagnostic support, and medical documentation, improving efficiency and interpretability in clinical workflows.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>Superbill</h3>
              <!--</a>-->
			  <h6> Health Insurance </h6>
			  <b>Tech Stack: Python, OCR (Tesseract), NLP (SpaCy, NLTK), AWS</b>
              <p>This is a MultiModel Project with Computer Vision and Natural Language Processing for Exela Technologies that aimed at Extract data from Superbill Health Insurance from with variety of structure with handwritten Images using OCR (Optical Character Recognition) Technique and Collect valuable information using Natural Language Processing (Spacy and NLTK). We process data to identify plan rates and benefits, plan benefits relate to plan rates, plan rates vary by age, plans vary across insurance network providers. It was one-year project, and we were able to successfully develop this project on AWS server and achieve excellent response from the customer.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>BankMate</h3>
              <!--</a>-->
			  <h6> Finance </h6>
			  <b>Tech Stack: Python, SQL, Docker, Deep Learning (Convolutional neural network (CNN) OCR + NLP + ML), AWS</b>
              <p>Implemented algorithms convolutional neural network (CNN) to convert Image and PDF of contract documents, Bank Statements, Receipts, and invoice Documents to Text Format and extract valuable details from documents using Natural Language Processing, analyse the extracted data to perform Fraud Detection and designing and implementing data pipelines using batch data transfers. I completed this task using Python, SQL, Docker, Machine Learning deployed the product on AWS, managed model with version control and prepared the visualisation dashboard using PowerBI and contributed to development of web application development. During this project, I got opportunity to work in agile environment and collaborate with stakeholders and team members and present the results to non-technical audience in the meeting.</p>
            </div>
          </div>
		  
		<div> </div>
        </div>
		
		<div class="row gy-4">
		
		<div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
			<div class="service-item item-cyan position-relative">
			  <!--<a href="#" class="stretched-link">--> <!--https://github.com/AnjaliDharmik/Fake-News-Detection-->
				<h3>Fake-News-Detection</h3>
			  <!--</a>-->
			  <h6> Surveillance </h6>
			  <b>Tech Stack: Python, NLP (TF-IDF, Word Embeddings), ML (Logistic Regression, Naive Bayes, Random Forest)</b>
			  <p>The Fake News Detection project utilizes machine learning and natural language processing (NLP) techniques to automatically identify and classify news articles as genuine or fake. Using the Fake and Real News Articles dataset, the model analyzes linguistic features and textual patterns to detect deceptive content. This system aims to enhance information credibility by providing an automated mechanism for verifying news authenticity, thereby helping to mitigate the spread of misinformation across digital platforms.</p>
			</div>
		  </div>
			  
		</div>

      </div>
	  
	  </section>
	  
	  <section id="ML" class="services section">
	  
      <div class="container section-title" data-aos="fade-up">
        <h2>Machine Learning Peojects</h2>
      </div>
	  
	  <div class="container">

        <div class="row gy-4">

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>Lead Scoring</h3>
              <!--</a>-->
			  <h6> Marketing </h6>
			  <b>Tech Stack: Python, PySpark, Databricks, CatBoost, MLflow, Eloqua and Microsoft Dynamics APIs</b>
              <p>Lead Scoring is a Machine Learning based model to classify and prioritize leads as qualified or disqualified. Built scalable feature engineering pipelines using behavioural, demographic, and engagement data, and applied CatBoost classification to enhance prediction accuracy. Implemented on Big Data platforms (PySpark, Databricks) with real-time deployment via MLflow, including monitoring through business and technical KPIs with comparative visualizations. Integrated seamlessly with the Eloqua marketing and sales platform, enabling data-driven lead prioritization, higher conversion rates, and improved marketing sales alignment.</p>
            </div>
          </div>
		  
		  <!--Marketing Mix Model-->
		  
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/AIRLINE-REVIEW-ANALYSIS" class="stretched-link">
                <h3>Airline Review Analysis</h3>
              </a>
			  <h6> Marketing </h6>
			  <b>Tech Stack: Python, BeautifulSoup, NLP (Topic Modeling, Sentiment Analysis), Visualization (Matplotlib, WordCloud) </b>
              <p>The Airline Review Analysis project involved performing web scraping and text analysis on British Airways customer reviews to extract actionable insights. Using Python (BeautifulSoup, Jupyter Notebook), I collected and cleaned large volumes of review data from airlinequality.com. I then applied Natural Language Processing (NLP) techniques, including topic modeling, sentiment analysis, and word cloud visualization, to identify key themes and customer sentiment trends. The findings were summarized and presented through a PowerPoint report, providing data-driven insights to improve customer experience and service quality.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/Credit-Card-Fraud-Detection" class="stretched-link">
                <h3>Credit Card Fraud Detection</h3>
              </a>
			  <b>Tech Stack: Python, Logistic Regression, Random Forest, XGBoost, CatBoost, LightGBM </b>
              <p>Credit Card Fraud Detection System for real time transaction data of financial services and enhance the security of its online transaction platform due to a growing number of customers experiencing unexpected fund disappearances or unauthorized transfers. I worked in Collaborative Team Environment with Data Team of 6 Data Scientist, Security and IT Teams for 6 months. We used Logistic Regression, Random Forest, XGBoost, Catboost, and LightGBM Algorithms to Model Building on Customer Provided real-time data of 12,000+ customers daily. Whereas approximately 140,000 transactions that occurred between October 2018 and April 2019. There were around 130,000 normal transactions and only 6% of them were fraudulent. We addressed the problem of an imbalanced dataset with various techniques such as data oversampling (augmenting the existing data samples) and data sample generation. After 6 months of the model development Customer benefits in Cost Reduction and Customer Satisfaction. Our client’s online card transaction platform became a safer service and gained more loyalty from their customers.</p>
            </div>
          </div>
		  <div> </div>

        </div>
		
		<div class="row gy-4">

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/Superstore-Sales-Forecasting" class="stretched-link">
                <h3>Superstore Sales Forecasting</h3>
              </a>
			  <h6> Sales </h6>
			  <b>Tech Stack: Python, Time Series (SARIMA), Matplotlib, Seaborn, Plotly</b>
              <p>Performed exploratory data analysis on a super store’s data to gain useful insights about the store’s sales. Visualization tools like Matplotlib, Seaborn and Plotly were used to display the information obtained by Performing EDA. Trained a time series analysis model, SARIMA using the pre-processed data to predict the future sales of the store.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
			<div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/DemandForecasting" class="stretched-link">
                <h3>Demand Forecasting</h3>
              </a>
			  <h6> Supply Chain </h6>
			  <b>Tech Stack: Python, SQL, MLFlow, Azure</b>
              <p>Occupancy Prediction for Effective Linen Supply Management: Implemented an end-to-end product demand forecasting solution for a company, utilizing historical sales data using Python, SQL and Deployed the forecasting model on the Azure cloud platform using MLFlow, enabling real-time demand predictions for inventory planning.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/property_price_analysis" class="stretched-link">
                <h3>Property First listing Price Forecasting</h3>
              </a>
			  <h6> Pricing </h6>
			  <b>Tech Stack: Python, Pandas, Scikit-learn, EDA, Feature Engineering</b>
              <p>As a Data Scientist at Airbnb, I developed a price prediction model to help new hosts set competitive listing prices. Using the Airbnb London listings dataset, which included property details, amenities, and listing parameters such as minimum nights, accommodates, and availability, I performed data preprocessing, exploratory data analysis (EDA), and feature engineering. The objective was to build a predictive model that estimates the optimal price for new listings based on property characteristics and location, enabling hosts to make data-driven pricing decisions in a competitive market.</p>
            </div>
          </div>
		  
		  <div> </div>

        </div>
		
		<div class="row gy-4">
		
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/Product-Predict-the-price" class="stretched-link">
                <h3>Product Price Forecast</h3>
              </a>
			  <h6> Pricing </h6>
			  <b>Tech Stack: Python, Regression Models, Scikit-learn</b>
              <p>Halloween is a night of costumes, fun, and candy that takes place every year on October 31. On this day people dress up in various costumes that have a scary overtone and go trick-or-treating to gather candy. This year, on Halloween, there is a carnival in your neighborhood. Besides the various games, there are also 50 stalls that are selling various products, which fall under various categories. The task is to predict the selling price of the products based on the provided features.</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/Predict-the-employee-burn-out-rate" class="stretched-link">
                <h3>Predict the Employee Burn Out Rate</h3>
              </a>
			  <b>Tech Stack: Python, ML, HR Analytics</b>
              <p>The Employee Burnout Prediction project aimed to assess and predict employee burnout levels using machine learning techniques to support workplace mental health initiatives. By analyzing employee-related features such as workload, work hours, stress levels, and performance metrics, the model identified key factors contributing to burnout risk. The predictive system enables organizations to proactively monitor employee well-being and implement data-driven strategies to reduce stress and improve productivity.</p>
            </div>
          </div>
		  
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/Home-Loan-Space" class="stretched-link">
                <h3>Home Loan Space</h3>
              </a>
			  <h6> Finance </h6>
			  <b>Tech Stack: Python, SQL, AutoML, Machine Learning</b>
              <p>As a Data Scientist at Standard Bank, I have implemented Machine Learning model using bespoke and AutoML with SQL, Python to assess the creditworthiness of an applicant and predict if the potential borrower will default on his/her loan or not and do this such that they receive a response within few seconds.</p>
            </div>
          </div>
		  
		  <div> </div>

        </div>
		
		<div class="row gy-4">

          <!--<div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/Techniques" class="stretched-link">
                <h3>Balancing Data</h3>
              </a>
              <p>**</p>
            </div>
          </div>-->

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <a href="https://github.com/AnjaliDharmik/IncomeLevelClassification" class="stretched-link">
                <h3>Income Level Classification</h3>
              </a>
			  <b>Tech Stack: Python, Scikit-learn, Binary Classification</b>
              <p>The main goal is to perform binary classification analysis to predict whether a person income level over 50K a year or not using census bureau database.</p>
            </div>
          </div>
		
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>Energy Demand Forecasting for UK</h3>
              <!--</a>-->
			  <h6> Energy </h6>
			  <b>Tech Stack: Python, Time Series (SARIMA, XGBoost, Prophet, LSTM), Azure SQL Server</b>
              <p>The Objective of this project to Implement time series forecasting to predict the energy demand in the Great Britain.I have used the historic electricity demand data between 2009 and 2022 from the UK National Grid, which is collected using the API of the National Grid ESO and stored in AZURE SQL SERVER with the data has 257422 records with 20 features.Performed Exploratory Data Analysis to gain useful insights and Feature Engineering to clean the dataset, add new features to improve the prediction and understand the trend and seasonality of the Energy demand. Trained a time series analysis model, SARIMA,XGBoost,Linear Trees models,Prophet,LSTM and deep LSTM recurrent networks, using the pre-processed data to predict the future demand of the energy.</p>
            </div>
          </div>
		  
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>PowerDesk Edge - UK Energy Trading Analysis</h3>
              <!--</a>-->
			  <h6> Energy </h6>
			  <b>Tech Stack: Python, XGBoost, LSTM, Regression, Web Scraping (Nord Pool), Time Series</b>
              <p>Developed a trading algorithm to maximize profits from financial trades between two auctions of the day-ahead market in Great Britain's Energy Market. Data is collected from Nord Pool using web scraping and Applied Exploratory Data Analysis and Performed Preprocessing to gain the insights. Second part is to forecast price for first auction using Auction Data which has actual prices and second auction price using Demand Forecast Data, Performed Time Series Analysis using Linear Regression, XGBoost, LSTM and deep LSTM recurrent networks traded volumes in python and determined the optimal trading strategy.</p>
            </div>
          </div>
		  
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>Ventilation Mask Leakage Analysis</h3>
              <!--</a>-->
			  <h6> Healthcare </h6>
			  <b>Tech Stack: Python, Predictive Modeling, Regression</b>
              <p>To identify the leakage percentage of oxygen ventilation cycle of adults and children by applying predictive machine learning modelling to identify the leakage percentage based on real-time data.</p>
            </div>
          </div>

        </div>

      </div>
	  	  
    </section>
	
	<section id="LLM" class="services section">

      <div class="container section-title" data-aos="fade-up">
        <h2>Large Language Models Peojects</h2>
      </div>

      <div class="container">

        <div class="row gy-4">

          <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <!--<a href="#" class="stretched-link">-->
                <h3>NEO - Next-Generation Marketing Intelligence Platform</h3>
              <!--</a>-->
			  <h6> Marketing </h6>
			  <b>Tech Stack: Python, SQLCoder, LLaMA, Generative AI, Quantization, Finetuning</b>
              <p>Designed and deployed state-of-the-art Generative AI models (finetuning, quantization, and evaluation) to power NEO, HP’s next-generation marketing intelligence platform. Integrated SQLCoder, LLaMA, and code generation capabilities to transform insight discovery, delivering data, charts, and analytic insights that boost marketing productivity and unlock strategic decision-making. Partnered with HP Marketing stakeholders to scope, prototype, and build innovative AI solutions for both cloud platforms, while leading cross-functional collaboration to deliver complex, enterprise-scale projects.</p>
            </div>
          </div>
		  
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <h3>HR Chatbot – Gweedo</h3>
			  <h6> HR</h6>
			  <b>Tech Stack: Python, LangChain, OpenAI API, Hugging Face, AWS (S3, EC2), Flask, HTML/CSS</b>
              <p>Developed an AI-powered HR chatbot to automate employee support tasks such as policy queries, on-boarding, and off-boarding. Integrated with AWS for document retrieval and deployed a secure, web-based interface with authentication and feedback features. Ensured compliance with EU AI Act and GDPR by implementing transparency, data privacy, and human oversight mechanisms. Fine-tuned LLM models using company data, optimized performance through HR team feedback, and delivered full documentation and demo for deployment.</p>
            </div>
          </div>
		  
		  <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
            <div class="service-item item-cyan position-relative">
              <h3>Question-Answer Chatbot for US Government County using Google Cloud Vertex AI</h3>
			  <h6>Government</h6>
			  <b>Tech Stack: Google Cloud Vertex AI, Document AI, PaLM-2, Python, OCR, NLP, Model Garden, Vertex AI Search, Kubernetes, Firestore</b>
              <p>Developed an AI-powered Question-Answer chatbot for a US government county using Google Cloud Vertex AI. The system extracts and answers queries from contract PDF documents by integrating Document AI OCR, custom text normalization, and fine-tuned PaLM-2 language models. Implemented secure VPC networking, Vertex AI Workbench for model training, and Vertex AI Search for contextual grounding. Enhanced accuracy through prompt engineering, data preprocessing, and optimized hyperparameters, enabling reliable, context-aware responses for government document understanding.</p>
            </div>
          </div>
		  
		  

        </div>

      </div>
	  
	  </section>
	

  </main>

  <footer id="footer" class="footer light-background">

    <div class="container">
      <div class="copyright text-center ">
        <p>© <span>Copyright</span> <strong class="px-1 sitename">ANJALI DHARMIK</strong> <span>All Rights Reserved<br></span></p>
      </div>
    </div>

  </footer>

  <!-- Scroll Top -->
  <a href="#" id="scroll-top" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Preloader -->
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>

  <!-- Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>